---
title: "2b Trajectory Analysis"
author: "Brit Henderson"
date: "2/18/2022"
output: html_document
---

# testing crimCV package
```{r}
Examples
# Loads crimCV into the interpreter
library(crimCV)
# Load the "divide-and-round" TO1 dataset
data(TO1adj)
# Fit a 2 component ZIP(tau) model of degree 2. Here the CVE is not
# calculated and only ~1/4 of the data is used so that the code will
# run quickly enough to satisfy CRAN's package policies. To compute
# CVE run as:
# out1<-crimCV(TO1adj,2,dpolyp=2,rcv=TRUE)
subTO1adj<-TO1adj[1:100,]
out1<-crimCV(subTO1adj,2,dpolyp=2,init=5)
out2<-crimCV(subTO1adj,3,dpolyp=2,init=5)
out3<-crimCV(subTO1adj,4,dpolyp=2,init=5)

# Plot the component trajectories
plot(out1)
plot(out2)
# Print out some useful output
summary(out3)
```

#trying andrew wheeler's loop
```{r}
results <- c()  #initializing a set of empty lists to store the seperate models
measures <- data.frame(cbind(groups=c(),llike=c(),AIC=c(),BIC=c(),CVE=c())) #nicer dataframe to check out model 
                                                                            
#model selection diagnostics
max <- 4 #this is the number of grouping solutions to check

#looping through models
for (i in 1:max){
    model <- crimCV(subTO1adj,i,rcv=TRUE,dpolyp=3,dpolyl=3)
    results <- c(results, list(model))
    measures <- rbind(measures,data.frame(cbind(groups=i,llike=model$llike,
                                          AIC=model$AIC,BIC=model$BIC,CVE=model$cv)))
    #save(measures,results,file=paste0("Traj",as.character(i),".RData")) #save result
    }
#table of the model results
measures
plot(out3)
```

# Testing some of Andrew Wheeler's helper functions
```{r}
library(crimCV)
library(ggplot2)

long_traj <- function(model,data){
  df <- data.frame(data)
  vars <- names(df)
  prob <- model['gwt'] #posterior probabilities
  df$GMax <- apply(prob$gwt,1,which.max) #which group # is the max
  df$PMax <- apply(prob$gwt,1,max)       #probability in max group
  df$Ord <- 1:dim(df)[1]                 #Order of the original data
  prob <- data.frame(prob$gwt)
  names(prob) <- paste0("G",1:dim(prob)[2]) #Group probabilities are G1, G2, etc.
  longD <- reshape(data.frame(df,prob), varying = vars, v.names = "y", 
                   timevar = "x", times = 1:length(vars), 
                   direction = "long") #Reshape to long format, time is x, y is original count data
  return(longD)                        #GMax is the classified group, PMax is the probability in that group
}


weighted_means <- function(model,long_data){
  G_names <- paste0("G",1:model$ng)
  G <- long_data[,G_names]
  W <- G*long_data$y                                    #Multiple weights by original count var
  Agg <- aggregate(W,by=list(x=long_data$x),FUN="sum")  #then sum those products
  mass <- colSums(model$gwt)                            #to get average divide by total mass of the weight
  for (i in 1:model$ng){
    Agg[,i+1] <- Agg[,i+1]/mass[i]
  }
  long_weight <- reshape(Agg, varying=G_names, v.names="w_mean",
                         timevar = "Group", times = 1:model$ng, 
                         direction = "long")           #reshape to long
  return(long_weight)
}
  
pred_means <- function(model){
    prob <- model$prob               #these are the model predicted means
    Xb <- model$X %*% model$beta     #see getAnywhere(plot.dmZIPt), near copy
    lambda <- exp(Xb)                #just returns data frame in long format
    p <- exp(-model$tau * t(Xb))
    p <- t(p)
    p <- p/(1 + p)
    mu <- (1 - p) * lambda
    t <- 1:nrow(mu)
    myDF <- data.frame(x=t,mu)
    long_pred <- reshape(myDF, varying=paste0("X",1:model$ng), v.names="pred_mean",
                         timevar = "Group", times = 1:model$ng, direction = "long")
    return(long_pred)
}

#Note, if you estimate a ZIP model instead of the ZIP-tau model
#use this function instead of pred_means
pred_means_Nt <- function(model){
    prob <- model$prob               #these are the model predicted means
    Xb <- model$X %*% model$beta     #see getAnywhere(plot.dmZIP), near copy
    lambda <- exp(Xb)                #just returns data frame in long format
	Zg <- model$Z %*% model$gamma
    p <- exp(Zg)
    p <- p/(1 + p)
    mu <- (1 - p) * lambda
    t <- 1:nrow(mu)
    myDF <- data.frame(x=t,mu)
    long_pred <- reshape(myDF, varying=paste0("X",1:model$ng), v.names="pred_mean",
                         timevar = "Group", times = 1:model$ng, direction = "long")
    return(long_pred)
}

occ <- function(long_data){
 subdata <- subset(long_data,x==1)
 agg <- aggregate(subdata$PMax,by=list(group=subdata$GMax),FUN="mean")
 names(agg)[2] <- "AvePP" #average posterior probabilites
 agg$Freq <- as.data.frame(table(subdata$GMax))[,2]
 n <- agg$AvePP/(1 - agg$AvePP)
 p <- agg$Freq/sum(agg$Freq)
 d <- p/(1-p)
 agg$OCC <- n/d #odds of correct classification
 agg$ClassProp <- p #observed classification proportion
 #predicted classification proportion
 agg$PredProp <- colSums(as.matrix(subdata[,grep("^[G][0-9]", names(subdata), value=TRUE)]))/sum(agg$Freq) 
 #Jeff Ward said I should be using PredProb instead of Class prop for OCC
 agg$occ_pp <- n/ (agg$PredProp/(1-agg$PredProp))
 return(agg)
}
```

# Checking out Andrew Wheeler's plotting code
Now most effort seems to be spent on using model selection criteria to pick the number of groups, what may be called relative model comparisons. Once you pick the number of groups though, you should still be concerned with how well the model replicates the data at hand, e.g. absolute model comparisons. The graphs that follow help assess this. First we will use our helper functions to make three new objects. The first function, long_traj, takes the original model object, out1, as well as the original matrix data used to estimate the model, TO1adj. The second function, weighted_means, takes the original model object and then the newly created long_data longD. The third function, pred_means, just takes the model output and generates a data frame in wide format for plotting (it is the same underlying code for plotting the model).

```{r}
longD <- long_traj(model=out1,data=TO1adj)
x <- weighted_means(model=out1,long_data=longD)
pred <- pred_means(model=out1)

```

#We can subsequently use the long data longD to plot the individual trajectories faceted by their assigned groups. I have an answer on cross validated that shows how effective this small multiple design idea can be to help disentangle complicated plots.


#plot of individual trajectories in small multiples by group
```{r}
p <- ggplot(data=longD, aes(x=x,y=y,group=Ord)) + geom_line(alpha = 0.1) + facet_wrap(~GMax)
p 

Plotting the individual trajectories can show how well they fit the predicted model, as well as if there are any outliers. You could get more fancy with jittering (helpful since there is so much overlap in the low counts) but just plotting with a high transparency helps quite abit. This second graph plots the predicted means along with the weighted means. What the weighted_means function does is use the posterior probabilities of groups, and then calculates the observed group averages per time point using the posterior probabilities as the weights.
```

#plot of predicted values + weighted means
```{r}
p2 <- ggplot() + geom_line(data=pred, aes(x=x,y=pred_mean,col=as.factor(Group))) + 
                 geom_line(data=x, aes(x=x,y=w_mean,col=as.factor(Group))) + 
                geom_point(data=x, aes(x=x,y=w_mean,col=as.factor(Group)))
p2
```

Here you can see that the estimated trajectories are not a very good fit to the data. Pretty much eash series has a peak before the predicted curve, and all of the series except for 2 don’t look like very good candidates for polynomial curves.

It ends up that often the weighted means are very nearly equivalent to the unweighted means (just aggregating means based on the classified group). In this example the predicted values are a colored line, the weighted means are a colored line with superimposed points, and the non-weighted means are just a black line.

#predictions, weighted means, and non-weighted means
```{r}
nonw_means <- aggregate(longD$y,by=list(Group=longD$GMax,x=longD$x),FUN="mean")
names(nonw_means)[3] <- "y"

p3 <- p2 + geom_line(data=nonw_means, aes(x=x,y=y), col='black') + facet_wrap(~Group)
p3

```


You can see the non-weighted means are almost exactly the same as the weighted ones. For group 3 you typically need to go to the hundredths to see a difference.

#check out how close
```{r}
nonw_means[nonw_means$Group==3,'y'] -  x[x$Group==3,'w_mean']
#You can subsequently superimpose the predicted group means over the individual trajectories as well.

#superimpose predicted over ind trajectories
pred$GMax <- pred$Group
p4 <- ggplot() + geom_line(data=pred, aes(x=x,y=pred_mean), col='red') + 
                 geom_line(data=longD, aes(x=x,y=y,group=Ord), alpha = 0.1) + facet_wrap(~GMax)
p4
```


Two types of absolute fit measures I’ve seen advocated in the past are the average maximum posterior probability per group and the odds of correct classification. The occ function calculates these numbers given two vectors (one of the max probabilities and the other of the group classifications). We can get this info from our long data by just selecting a subset from one time period. Here the output at the console shows that we have quite large average posterior probabilities as well as high odds of correct classification. (Also updated to included the observed classified proportions and the predicted proportions based on the posterior probabilities. Again, these all show very good model fit.) Update: Jeff Ward sent me a note saying I should be using the predicted proportion in each group for the occ calculation, not the assigned proportion based on the max. post. prob. So I have updated to include the occ_pp column for this, but left the old occ column in as a paper trail of my mistake.
```{r}
occ(longD)



A plot to accompany this though is a jittered dot plot showing the maximum posterior probability per group. You can here that groups 3 and 4 are more fuzzy, whereas 1 and 2 mostly have very high probabilities of group assignment.

#plot of maximum posterior probabilities
subD <- longD[x==1,]
p5 <- ggplot(data=subD, aes(x=as.factor(GMax),y=PMax)) + geom_point(position = "jitter", alpha = 0.2)
p5
```

Remember that these latent class models are fuzzy classifiers. That is each point has a probability of belonging to each group. A scatterplot matrix of the individual probabilities will show how well the groups are separated. Perfect separation into groups will result in points hugging along the border of the graph, and points in the middle suggest ambiguity in the class assignment. You can see here that each group closer in number has more probability swapping between them.

#scatterplot matrix
```{r}
library(GGally)
sm <- ggpairs(data=subD, columns=4:7)
sm
```


And the last time series plot I have used previously is a stacked area chart.

#stacked area chart
```{r}
nonw_sum <- aggregate(longD$y,by=list(Group=longD$GMax,x=longD$x),FUN="sum")
names(nonw_sum)[3] <- "y"
p6 <- ggplot(data=nonw_sum, aes(x=x,y=y,fill=as.factor(Group))) + geom_area(position='stack')
p6

```


#try running code on small N of obs
```{r}
#test.dat <- as.data.frame(analysis.dat2[1:180,1:47])

#test.matrix <-as.double(test.dat[,1:46])
#test.matrix2 <- data.matrix(test.dat, rownames.force = NA)

#test.dat <- analysis.mat1[304:400,] #cannot handle missing values...
test.dat2 <- analysis.mat1[300:400,]

# Fit a 2 component ZIP(tau) model of degree 2. Here the CVE is not
# calculated and only ~1/4 of the data is used so that the code will
# run quickly enough to satisfy CRAN's package policies. To compute
# CVE run as: # out1<-crimCV(TO1adj,2,dpolyp=2,rcv=TRUE)
#out1<-crimCV(test.dat,2,dpolyp=2,init=5, rcv=FALSE)

# Next time, try replacing NA with -1
test.dat2[is.na(test.dat2)]<- -1

testout1<-crimCV(test.dat2,2,dpolyp=2,init=5, rcv=TRUE)
testout2 <-crimCV(test.dat2,2,dpolyp=2,init=5, rcv=FALSE)
othertest <- as.data.frame(testout1)
save(testout1, file = "rda/testresults1.csv")
checkresults1 <- load("rda/testresults.RData")
check1 <- checkresults[1]
```

#Try running A.W.'s loop on small number of obs
```{r}

results <- c()  #initializing a set of empty lists to store the seperate models
measures <- data.frame(cbind(groups=c(),llike=c(),AIC=c(),BIC=c(),CVE=c())) #nicer dataframe to check out model 
                                                                            
#model selection diagnostics
max <- 6 #this is the number of grouping solutions to check

#looping through models
for (i in 1:max){
    model <- crimCV(test.dat2,i,rcv=FALSE,dpolyp=3,dpolyl=3)
    results <- c(results, list(model))
    measures <- rbind(measures,data.frame(cbind(groups=i,llike=model$llike,
                                          AIC=model$AIC,BIC=model$BIC,CVE=model$cv)))
   # save(measures,results,file=paste0("rda/Results2x3_",as.character(i),".RData")) #save result
    }
#table of the model results
measures3x3 <-measures
measures3x3

#Re-run best model and output results to object
best.mod1 <- crimCV(test.dat2,4,rcv=FALSE, dpolyp =3, dpolyl =3)
plot(best.mod1)


### repeat for 2x3                                                                

results <- c()  
measures <- data.frame(cbind(groups=c(),llike=c(),AIC=c(),BIC=c(),CVE=c())) 

max <- 6 

for (i in 1:max){
    model <- crimCV(test.dat2,i,rcv=FALSE,dpolyp=2,dpolyl=3)
    results <- c(results, list(model))
    measures <- rbind(measures,data.frame(cbind(groups=i,llike=model$llike,
                                          AIC=model$AIC,BIC=model$BIC,CVE=model$cv)))
    }
measures2x3 <-measures
measures2x3

best.mod1 <- crimCV(test.dat2,5,rcv=FALSE, dpolyp =2, dpolyl =3)
plot(best.mod1)


### repeat for 3x2                                                                
results <- c()  
measures <- data.frame(cbind(groups=c(),llike=c(),AIC=c(),BIC=c(),CVE=c()))  
                                                                            
max <- 6 

for (i in 1:max){
    model <- crimCV(test.dat2,i,rcv=FALSE,dpolyp=3,dpolyl=2)
    results <- c(results, list(model))
    measures <- rbind(measures,data.frame(cbind(groups=i,llike=model$llike,
                                          AIC=model$AIC,BIC=model$BIC,CVE=model$cv)))
    }
measures3x2 <-measures
measures3x2

best.mod2x3 <- crimCV(test.dat2,6,rcv=FALSE, dpolyp =2, dpolyl =3)
plot(best.mod2x3)

best.mod3x3 <- crimCV(test.dat2,4,rcv=FALSE, dpolyp =2, dpolyl =3)
plot(best.mod3x3)

best.mod3x2 <- crimCV(test.dat2,3,rcv=FALSE, dpolyp =2, dpolyl =3)
plot(best.mod3x2)


measures3x3
measures3x2
measures2x3

```



# Some alternative code from Paul Schneider
```{r}
# GBTM AND K-MEANS ANALYSIS - RAW CODE
#Paul Schneider
#2018-04-30

#https://raw.githubusercontent.com/bitowaqr/traj/master/raw_code.R
#https://github.com/bitowaqr/trajhttps://github.com/bitowaqr/traj

# Load functions
url = "https://raw.githubusercontent.com/bitowaqr/traj/master/Setup_n_functions.R"
source(url)

# prepare data

  # Data needs the following format:
    # rows = cases
    # columns = observations at time points
    # no time variables needed
  
  # Enter file path or select manually
    #test.data = read.csv("https://raw.githubusercontent.com/bitowaqr/traj/master/test.data.csv")
    
  # Otherwise select your own data:
     test.data = test.dat2
    
  # reducing the size of the demo data set
    # test.data = test.data[1:100,] # looking at a subset 
  
  # Missing values need to be coded as negative values for crimCV
    #test.data[is.na(test.data)] = -0.00001
  
  # data frame to matrix
    #df = as.matrix(test.data[,-1])
   # rownames(df) = test.data$ID
     df = test.dat2
  
  # resulting data set
    kable(head(df),format="markdown")

# GBTM CLUSTERING: crimCV

## set the grid of models to evaluate
  # Set: 
    n.cluster = c(1,2,3,4,5,6) # 1.which k's to evaluate, 
    p.poly = c(1,2,3) # 2. which p's to evaluate, 
    rcv = F # 3. do you want to run cross validation? T/F

## model evaluation
  
  # Run all models
    cv.eval.list = list()
    index = 0
    for(k in n.cluster)
    {
      sub.index = 0
      index = index + 1
          cv.eval.list[[index]] = list()
          names(cv.eval.list)[k] = paste("Groups",k,sep="_")
          for(p in p.poly)
            {
            print(cat("\n Running models for", n.cluster[k], "Groups, and",p.poly[p],"polynomials..."))
            cat("\n running k=",k,"poly=",p," \n")
            sub.index = sub.index + 1
            temp = crimCV(df,
                          ng = k,
                          dpolyp = p,
                          rcv = rcv,     
                          model = "ZIP"
                          )
        
            cv.eval.list[[index]][[sub.index]] = temp
        
            names(cv.eval.list[[index]])[sub.index] = paste("polynomial",p,sep="_")
            }
    }
    
## retrireve model evaluation results  
  
  # retrieve AIC, BIC and CV error for each of the models
  points.for.AIC.plot = points.for.BIC.plot = points.for.cv.plot = data.frame(x=NA,value=NA,cluster=NA)
  for(c in 1:length(cv.eval.list)){
    for(p in 1:length(cv.eval.list[[c]])){
      
  
      tryCatch({
          cv = ifelse(!is.null(cv.eval.list[[c]][[p]]$cv),cv.eval.list[[c]][[p]]$cv,NA)
          points.for.cv.plot = rbind(points.for.cv.plot,
                                     data.frame(x=p,value=cv ,cluster=c))
      }, error =function(e){})
      
      
      tryCatch({
          aic = ifelse(!is.null(cv.eval.list[[c]][[p]]$AIC),cv.eval.list[[c]][[p]]$AIC,NA)
          points.for.AIC.plot = rbind(points.for.AIC.plot,
                                      data.frame(x=p,value=aic,cluster=c))
      }, error =function(e){})
      
          tryCatch({
          bic = ifelse(!is.null(cv.eval.list[[c]][[p]]$BIC),cv.eval.list[[c]][[p]]$BIC,NA)
          points.for.BIC.plot = rbind(points.for.BIC.plot,
                                      data.frame(x=p,value=bic,cluster=c))
          }, error =function(e){})
      
    }}
  
  points.for.AIC.plot = points.for.AIC.plot[-1,]  
  points.for.BIC.plot = points.for.BIC.plot[-1,] 
  points.for.cv.plot = points.for.cv.plot[-1,]
  
  AIC.gbtm.plot = ggplot(points.for.AIC.plot) + 
    geom_line(aes(x=x,y=value,col=as.factor(cluster))) +
    #geom_point(aes(x=x,y=value,col=as.factor(cluster))) +
    geom_text(aes(x=x,y=value,col=as.factor(cluster),label=cluster),size=5) +
    xlab("Polynomial") +
    ylab("AIC")
  
  BIC.gbtm.plot = ggplot(points.for.BIC.plot) + 
    geom_line(aes(x=x,y=value,col=as.factor(cluster))) +
    # geom_point(aes(x=x,y=value,col=as.factor(cluster))) +
    geom_text(aes(x=x,y=value,col=as.factor(cluster),label=cluster),size=5) +
    xlab("Polynomial") +
    ylab("BIC")
  
  cv.error.gbtm.plot = ggplot(points.for.cv.plot) + 
    geom_line(aes(x=x,y=value,col=as.factor(cluster))) +
    geom_text(aes(x=x,y=value,col=as.factor(cluster),label=cluster),size=5) +
    #geom_point(aes(x=x,y=value,col=as.factor(cluster))) +
    xlab("Polynomial")  +
    ylab("LOOCV Absolute error") 
  
  plot.legend = get_legend(AIC.gbtm.plot + theme(legend.position = "bottom"))
  
  model.eval.plot = 
    plot_grid(
      plot_grid(cv.error.gbtm.plot + theme(legend.position = "none"),
                              BIC.gbtm.plot + theme(legend.position = "none"),
                              AIC.gbtm.plot + theme(legend.position = "none"),
                              ncol=3),
      plot.legend,nrow = 2,rel_heights = c(10,1))
  
  # plot model evaluation
  model.eval.plot

## Set the parameters for your model of choice

  # Select k and p 
  k.set = 5
  p.set = 3
  
  # plot details
  y.axis.label = "Young Adult Arrests per County Year"
  x.axis.label = "Year"
  plot.title = "Young Adult Arrest Rate by Year"


## Retrive data from your model of choice

  
  # retrieve the final model
  
    # select model
    ind.k = which(grepl(k.set,names(cv.eval.list)))
`    ind.p = which(grepl(p.set,names(cv.eval.list[[ind.k]])))
    # retrieve participants membership
    gbtm.members = data.frame(ID =  rownames(df),
                                   cluster = apply(summary(cv.eval.list[[ind.k]][[ind.p]]),
                                                   1,
                                                   function(x)which(x ==max(x))))
  
    members.per.cluster = data.frame(table(gbtm.members$cluster))

## Plot estimated trajectories

  
  # estimated trajectories
  
  modelled.list = plot(cv.eval.list[[ind.k]][[ind.p]],size=1,plot=F)
  modelled.list$time = modelled.list$time 
  
    model.plot.modelled = 
      ggplot(modelled.list) +
      geom_line(aes(x=time,y=value,col=cluster)) +
      scale_y_continuous(name=y.axis.label) +
      scale_x_continuous(name=x.axis.label) +
      ggtitle(plot.title) +
      scale_color_manual(lab=paste("Group ",members.per.cluster$Var1," (n=",members.per.cluster$Freq,")",sep=""),
                         values=c(2,3,4),
                         name="Estimated group trajectories") +
      theme_minimal()
    
    model.plot.modelled

## Retrieve group function terms

  
  # retrieve model function terms with intercept and * for p < .05
    long.test.dat = melt(df)
    names(long.test.dat)  = c("ID","time","value")
    long.test.dat$time = as.numeric(gsub("t.","",long.test.dat$time))
    long.test.dat = merge(long.test.dat,gbtm.members,"ID")
    long.test.dat$cluster = as.factor(long.test.dat$cluster)
  
    polynomial.model.results = summary(lm(value ~ -1+poly(time,p.set):cluster+cluster, long.test.dat))
    model.spec = round(polynomial.model.results$coefficients[,1],2)
    sig.model.specification = ifelse(polynomial.model.results$coefficients[,4]<0.05,"*"," ")
    model.spec = paste(model.spec,sig.model.specification,sep="")
    model.spec = formatC(model.spec)
    model.spec = matrix(data=model.spec, ncol=p.set+1)
    
    colnames(model.spec) = c("Intercept",paste("Polynomial",1:p.set))
    rownames(model.spec) = c(paste("Group",1:k.set))
    
    kable(model.spec,format="markdown")

## Plot average group trajectories
  
  # Retrieve observed group trajectories
    long.test.dat$value[long.test.dat$value<0] = NA
    long.test.dat.means = aggregate(value ~ cluster + time, long.test.dat, function(x) mean( x , na.rm = T))
    
    model.plot.from.data = ggplot(long.test.dat.means) +
      geom_line(aes(x=time,y=value,col=cluster)) +
      scale_y_continuous(name=y.axis.label) +
      scale_x_continuous(name=x.axis.label) +
      ggtitle(plot.title) +
      scale_color_manual(lab=paste("Group ",members.per.cluster$Var1," (n=",members.per.cluster$Freq,")",sep=""),
                         values=c(2,3,4),
                         name="Observed group trajectories") +
      theme_minimal()
    
    model.plot.from.data 

## Setup for complex plot

  # give names to clusters? 
  cluster.names = paste(
    c("First cluster", 
      "Second",
      "Third"),
      " (n=",format(as.numeric(members.per.cluster$Freq),digits=1),")",sep="")
  
  
  # set a y limits to have all sub plots on the same scale?
  set.y.limit = c(0,15000)


## Plot group and individual trajectories
  
  # Group average overview and individual trajectories
  
  pop.average.traj = aggregate(value ~time, long.test.dat, function(x) mean(x,na.rm=T))
  model.plot.modelled.plus.pop.average = 
      ggplot() +
    geom_line(data=modelled.list,aes(x=time,y=value,col=cluster,linetype="Estimated")) +
    geom_line(data=pop.average.traj,aes(x=time,y=value,col="Total",linetype="Average")) +  
      scale_y_continuous(name=y.axis.label) +
      scale_x_continuous(name=x.axis.label) +
      ggtitle(plot.title) +
      scale_color_manual(lab=c(paste("Group ",members.per.cluster$Var1," (n=",members.per.cluster$Freq,")",sep=""),
                               paste("Total", " (n=",sum(members.per.cluster$Freq),")",sep="")),
                         values=c(2:(k.set+1),1),
                         name="Estimated group trajectories") +
    scale_linetype_manual(lab=c("Average","Estimated"),values = c(2,1), name="")+
    guides(color = guide_legend(order = 1),
        linetype = guide_legend(order=0)) +
      theme_minimal()
  
  
    individual.plot.list = list()
    times.ex = unique(long.test.dat$time)
    for(i in 1:length(unique(long.test.dat$cluster))){
      individual.plot.list[[i]] = 
        ggplot() +
        theme_minimal() +
            geom_line(data=long.test.dat[long.test.dat$cluster==i,],
                      aes(x=time,y=value,group=ID,linetype="Average"),col=i+1,alpha=0.3,size=0.4) +
        geom_line(data=long.test.dat.means[long.test.dat.means$cluster==i,],
                      aes(x=time,y=value,linetype="Average"),col=i+1,size=1) +
         geom_line(data=modelled.list[modelled.list$cluster==i,],
                      aes(x=time,y=value,col=cluster,linetype="Estimate"),col=i+1,size=1) +
          scale_linetype_manual(lab=c("Average","Estimated"),values = c(2,1), name="") +
        theme(legend.position = "none") +
        ylab("") +
              # ggtitle(plot.titles.for.mega[i]) +
              coord_cartesian(ylim=set.y.limit) 
    }
    # individual.plot.list[[length(individual.plot.list)+1]] = get.legend
    
    gbtm.mega.plot = 
      plot_grid(model.plot.modelled.plus.pop.average+
                  coord_cartesian(ylim=set.y.limit),
                plot_grid(plotlist=individual.plot.list,ncol=round(k.set/2,0)),ncol=1,rel_heights = c(2,3))
    
    gbtm.mega.plot
    
```





# Checking out Andrew Wheeler's plotting code
"Now most effort seems to be spent on using model selection criteria to pick the number of groups, what may be called relative model comparisons. Once you pick the number of groups though, you should still be concerned with how well the model replicates the data at hand, e.g. absolute model comparisons. The graphs that follow help assess this. First we will use our helper functions to make three new objects. The first function, long_traj, takes the original model object, out1, as well as the original matrix data used to estimate the model, TO1adj. The second function, weighted_means, takes the original model object and then the newly created long_data longD. The third function, pred_means, just takes the model output and generates a data frame in wide format for plotting (it is the same underlying code for plotting the model).

```{r}
longD <- long_traj(model=best.mod1,data=test.dat2)
x <- weighted_means(model=best.mod1,long_data=longD)
pred <- pred_means(model=best.mod1)
#We can subsequently use the long data longD to plot the individual trajectories faceted by their assigned groups. I have an answer on cross validated that shows how effective this small multiple design idea can be to help disentangle complicated plots.


#plot of individual trajectories in small multiples by group
p <- ggplot(data=longD, aes(x=x,y=y,group=Ord)) + geom_line(alpha = 0.1) + facet_wrap(~GMax)
p 

#Plotting the individual trajectories can show how well they fit the predicted model, as well as if there are any outliers. You could get more fancy with jittering (helpful since there is so much overlap in the low counts) but just plotting with a high transparency helps quite abit. This second graph plots the predicted means along with the weighted means. What the weighted_means function does is use the posterior probabilities of groups, and then calculates the observed group averages per time point using the posterior probabilities as the weights.

#plot of predicted values + weighted means
p2 <- ggplot() + geom_line(data=pred, aes(x=x,y=pred_mean,col=as.factor(Group))) + 
                 geom_line(data=x, aes(x=x,y=w_mean,col=as.factor(Group))) + 
                geom_point(data=x, aes(x=x,y=w_mean,col=as.factor(Group))) +
                facet_grid(Group~.)
p2
```

Here you can see that the estimated trajectories are not a very good fit to the data. Pretty much each series has a peak before the predicted curve, and all of the series except for 2 don’t look like very good candidates for polynomial curves.



#predictions, weighted means, and non-weighted means
It ends up that often the weighted means are very nearly equivalent to the unweighted means (just aggregating means based on the classified group). In this example the predicted values are a colored line, the weighted means are a colored line with superimposed points, and the non-weighted means are just a black line. You can see the non-weighted means are almost exactly the same as the weighted ones. For group 3 you typically need to go to the hundredths to see a difference.

```{r}
nonw_means <- aggregate(longD$y,by=list(Group=longD$GMax,x=longD$x),FUN="mean")
names(nonw_means)[3] <- "y"

p3 <- p2 + geom_line(data=nonw_means, aes(x=x,y=y), col='black') + facet_wrap(~Group)
p3

#check out how close
nonw_means[nonw_means$Group==3,'y'] -  x[x$Group==3,'w_mean']
```



#superimpose predicted over ind trajectories
You can subsequently superimpose the predicted group means over the individual trajectories as well.
```{r}
pred$GMax <- pred$Group
p4 <- ggplot() + geom_line(data=pred, aes(x=x,y=pred_mean), col='red') + 
                 geom_line(data=longD, aes(x=x,y=y,group=Ord), alpha = 0.1) + facet_wrap(~GMax)
p4

```


#plot of maximum posterior probabilities

Two types of absolute fit measures I’ve seen advocated in the past are the average maximum posterior probability per group and the odds of correct classification. The occ function calculates these numbers given two vectors (one of the max probabilities and the other of the group classifications). We can get this info from our long data by just selecting a subset from one time period. Here the output at the console shows that we have quite large average posterior probabilities as well as high odds of correct classification. (Also updated to included the observed classified proportions and the predicted proportions based on the posterior probabilities. Again, these all show very good model fit.) Update: Jeff Ward sent me a note saying I should be using the predicted proportion in each group for the occ calculation, not the assigned proportion based on the max. post. prob. So I have updated to include the occ_pp column for this, but left the old occ column in as a paper trail of my mistake.
```{r}
occ(longD)
#A plot to accompany this though is a jittered dot plot showing the maximum posterior probability per group. You can here that groups 3 and 4 are more fuzzy, whereas 1 and 2 mostly have very high probabilities of group assignment.

subD <- longD[x==1,]
p5 <- ggplot(data=subD, aes(x=as.factor(GMax),y=PMax)) + geom_point(position = "jitter", alpha = 0.2)
p5
```


#scatterplot matrix
Remember that these latent class models are fuzzy classifiers. That is each point has a probability of belonging to each group. A scatterplot matrix of the individual probabilities will show how well the groups are separated. Perfect separation into groups will result in points hugging along the border of the graph, and points in the middle suggest ambiguity in the class assignment. You can see here that each group closer in number has more probability swapping between them.
```{r}
library(GGally)
options(scipen = 100)
sm <- ggpairs(data=subD, columns=4:7)
sm
```


#stacked area chart
And the last time series plot I have used previously is a stacked area chart.
```{r}
nonw_sum <- aggregate(longD$y,by=list(Group=longD$GMax,x=longD$x),FUN="sum")
names(nonw_sum)[3] <- "y"
p6 <- ggplot(data=nonw_sum, aes(x=x,y=y,fill=as.factor(Group))) + geom_area(position='stack')
p6
```









################################################################### extra  code
#convert data columns to numeric
```{r}

analysis.mat2 <- analysis.mat1
# Check columns classes
sapply(analysis.mat2, class)


i = c(1:46)
analysis.mat2[,i] <- apply(analysis.mat2[ , i], 2,            # Specify own function within apply
                    function(x) as.numeric(as.character(x)))
```



#some extra k-means clustering code from Paul Schneider
```{r}
# k-means clustering

## k-means of what?

  ?step1measures # info shows the 24 measurements on which k-means is performend

## Automated function for plotting results from `step1measures` --> `step2factors` --> `step3clusters`

  # Takes a 'data frame' and creates traj cluster analysis with mean and individual plots
      traj.k.mean = function( fill.data.matrix = test.data[,-1],
                              ID = test.data$ID ,
                              nclusters = NULL # set number of clusters, or NULL to let R decide
                         )
        {
        times = dim(fill.data.matrix)[2]
        colnames(fill.data.matrix) = paste("x",1:dim(fill.data.matrix)[2],sep="")
        time.mat =  matrix(data=rep(1:dim(fill.data.matrix)[2],each=dim(fill.data.matrix)[1]),
                           nrow=dim(fill.data.matrix)[1],
                           ncol=dim(fill.data.matrix)[2])
        colnames(time.mat) = paste("t",1:dim(fill.data.matrix)[2],sep="")
        cluster.data = cbind(ID = ID, fill.data.matrix,time.mat)
        
        fill.data.matrix = cbind(ID,fill.data.matrix)
        time.mat = cbind(ID,time.mat)
        
        cat("\n Clustering needas at least 5 observation per case... Cases with fewer observations are being removed...! \n")
        s1 = step1measures(Data = fill.data.matrix, 
                           Time = time.mat,
                           ID = T )
        s2all = step2factors(s1)
        s3all = step3clusters(s2all, nclusters = nclusters)  # 5 clusters
        # clust.build.plot(s3all,y.max.lim =y.max.lim)
        k.membership.table = data.frame(table(s3all$clusters$cluster))
        k.membership = s3all$clusters
        k.k = unique(k.membership$cluster)
        
        
        cluster.plot.df = data.frame(cluster=NA,value=NA,time=NA)
        for(i in k.k){
          mean.per.cluster = colMeans(fill.data.matrix[fill.data.matrix$ID %in% k.membership$ID[k.membership$cluster==i],-1],na.rm=T)
          cluster.name = paste(i," (",length(fill.data.matrix[fill.data.matrix$ID %in% k.membership$ID[k.membership$cluster==i],1]),")",sep="")
          cluster.plot.df=rbind(cluster.plot.df,
                                data.frame(cluster=cluster.name,
                                           value=mean.per.cluster,
                                           time = 1:length(mean.per.cluster)))
        }
        cluster.plot.df = cluster.plot.df[-1,]
        cluster.plot.df$time = as.numeric(cluster.plot.df$time)
        n.per.cluster = as.numeric(table(cluster.plot.df$cluster))
        cluster.plot.df$cluster = as.factor(cluster.plot.df$cluster)
        
        mean.cluster.plot = 
          ggplot(cluster.plot.df) + 
          geom_line(aes(x=time,y=value,col=cluster)) +
          guides(color=guide_legend("Cluster (n)"))  # add guide properties by aesthetic
        
        # Individual plot
        indiv.data = merge(fill.data.matrix,k.membership,"ID")
        indiv.data = melt(indiv.data,id.vars=c("ID","cluster"))
        indiv.data$variable = as.character(indiv.data$variable)
        indiv.data$variable = gsub("x","",indiv.data$variable)
        indiv.data$variable = as.numeric(indiv.data$variable)
        indiv.data$cluster = as.factor(indiv.data$cluster)
        
        lev.clust = levels(indiv.data$cluster)
        n.clust = as.numeric(by(indiv.data$ID,indiv.data$cluster,function(x)length(unique(x))))
        names.clust = data.frame(cluster = lev.clust,n = paste(lev.clust," (",n.clust,")",sep=""))
        indiv.data = merge(indiv.data,names.clust,by="cluster")
        indiv.data$ID = as.factor(indiv.data$ID)
  
        individual.cluster.plot = 
          ggplot(indiv.data,aes(x=variable,y=value,col= cluster)) + 
              geom_line(aes(group=ID),alpha=0.3,size=0.6) +
              stat_summary(aes(group = cluster), fun.y = mean, geom = 'line', size=1, alpha=1) +
              guides(color=guide_legend("Cluster")) 
        
        
        cluster.analysis = list(
          s1 = s1,
          s2all = s2all,
          s3all = s3all,
          membership = k.membership,
          mean.cluster.plot=mean.cluster.plot,
          individual.cluster.plot = individual.cluster.plot)
        
        return(cluster.analysis)
      }

## k-means cluster analysis
  
  cluster.analysis.full = traj.k.mean( fill.data.matrix = test.data[,-1], ID = test.data$ID ,nclusters = NULL)
  
  # results
  cluster.analysis.full$s3
  cluster.analysis.full$mean.cluster.plot
  cluster.analysis.full$individual.cluster.plot
  
## k-means clusters within GBTM clusters

  # k-means clusters within GBTM clusters
  unique.GBTM.clusters = unique(long.test.dat$cluster)
  
  clusters.within.clusters.plots = list()
  set.y.limit = set.y.limit 
  
  for(k in unique.GBTM.clusters){
    temp.pat = unique(long.test.dat$ID[long.test.dat$cluster==k])
    temp.fill.data = test.data[test.data$ID %in% temp.pat,-1]
    temp.k.means = traj.k.mean( fill.data.matrix = temp.fill.data, ID = temp.pat ,nclusters = NULL)
    clusters.within.clusters.plots[[k]] = temp.k.means$individual.cluster.plot + coord_cartesian(ylim=set.y.limit) 
  }


## Overiew plot

  plot_grid(plotlist = clusters.within.clusters.plots)


## References

  citation("crimCV")
  citation("traj")

## end

```
